{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "from preprocessing import *\n",
    "from plots import *\n",
    "#from crossval import *\n",
    "from logreg import *\n",
    "from split_data import *\n",
    "from classification_accuracy import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from implementations import *\n",
    "from build_polynomial import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "DATA_TRAIN_PATH = 'C:/Users/joeld/Desktop/EPFL/machine learning/AIAIaie/data/train.csv' # TODO: download train data and supply path here \n",
    "#DATA_TRAIN_PATH = '/Users/benoithohl/Desktop/epfl/master_epfl/Ma3/Machine_learning/AIAIaie/data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>DER_pt_ratio_lep_tau</th>\n",
       "      <th>DER_met_phi_centrality</th>\n",
       "      <th>DER_lep_eta_centrality</th>\n",
       "      <th>PRI_tau_pt</th>\n",
       "      <th>PRI_tau_eta</th>\n",
       "      <th>PRI_tau_phi</th>\n",
       "      <th>PRI_lep_pt</th>\n",
       "      <th>PRI_lep_eta</th>\n",
       "      <th>PRI_lep_phi</th>\n",
       "      <th>PRI_met</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>s</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.396</td>\n",
       "      <td>0.200</td>\n",
       "      <td>32.638</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.381</td>\n",
       "      <td>51.626</td>\n",
       "      <td>2.273</td>\n",
       "      <td>-2.414</td>\n",
       "      <td>16.824</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>b</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>0.879</td>\n",
       "      <td>1.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>42.014</td>\n",
       "      <td>2.039</td>\n",
       "      <td>-3.011</td>\n",
       "      <td>36.918</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.103</td>\n",
       "      <td>44.704</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>b</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>3.776</td>\n",
       "      <td>1.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>32.154</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>-2.093</td>\n",
       "      <td>121.409</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>1.052</td>\n",
       "      <td>54.283</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>b</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>2.354</td>\n",
       "      <td>-1.285</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>22.647</td>\n",
       "      <td>-1.655</td>\n",
       "      <td>0.010</td>\n",
       "      <td>53.321</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-3.100</td>\n",
       "      <td>31.082</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>b</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-1.385</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>28.209</td>\n",
       "      <td>-2.197</td>\n",
       "      <td>-2.231</td>\n",
       "      <td>29.774</td>\n",
       "      <td>0.798</td>\n",
       "      <td>1.569</td>\n",
       "      <td>2.723</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100005</td>\n",
       "      <td>b</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>278.876</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.975</td>\n",
       "      <td>53.651</td>\n",
       "      <td>0.371</td>\n",
       "      <td>1.329</td>\n",
       "      <td>31.565</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>1.857</td>\n",
       "      <td>40.735</td>\n",
       "      <td>2.237</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100006</td>\n",
       "      <td>s</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>305.967</td>\n",
       "      <td>3.371</td>\n",
       "      <td>1.393</td>\n",
       "      <td>0.791</td>\n",
       "      <td>28.850</td>\n",
       "      <td>1.113</td>\n",
       "      <td>2.409</td>\n",
       "      <td>97.240</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>38.421</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100007</td>\n",
       "      <td>s</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>138.178</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-1.305</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>78.800</td>\n",
       "      <td>0.654</td>\n",
       "      <td>1.547</td>\n",
       "      <td>28.740</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-1.347</td>\n",
       "      <td>22.275</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100008</td>\n",
       "      <td>b</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>65.333</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-1.366</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>39.008</td>\n",
       "      <td>2.433</td>\n",
       "      <td>-2.532</td>\n",
       "      <td>26.325</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1.884</td>\n",
       "      <td>37.791</td>\n",
       "      <td>0.024</td>\n",
       "      <td>129.804</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100009</td>\n",
       "      <td>s</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>28.859</td>\n",
       "      <td>255.123</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>54.646</td>\n",
       "      <td>-1.533</td>\n",
       "      <td>0.416</td>\n",
       "      <td>32.742</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>132.678</td>\n",
       "      <td>0.845</td>\n",
       "      <td>294.741</td>\n",
       "      <td>1</td>\n",
       "      <td>167.735</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>167.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100010</td>\n",
       "      <td>b</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>86.240</td>\n",
       "      <td>79.692</td>\n",
       "      <td>27.201</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.338</td>\n",
       "      <td>27.201</td>\n",
       "      <td>81.734</td>\n",
       "      <td>1.750</td>\n",
       "      <td>-1.412</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>29.718</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>2.878</td>\n",
       "      <td>52.016</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>51.276</td>\n",
       "      <td>0.688</td>\n",
       "      <td>250.178</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100011</td>\n",
       "      <td>b</td>\n",
       "      <td>114.744</td>\n",
       "      <td>10.286</td>\n",
       "      <td>75.712</td>\n",
       "      <td>30.816</td>\n",
       "      <td>2.563</td>\n",
       "      <td>252.599</td>\n",
       "      <td>-1.401</td>\n",
       "      <td>2.888</td>\n",
       "      <td>36.745</td>\n",
       "      <td>239.804</td>\n",
       "      <td>1.061</td>\n",
       "      <td>1.364</td>\n",
       "      <td>0.769</td>\n",
       "      <td>35.976</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>38.188</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>2.502</td>\n",
       "      <td>22.385</td>\n",
       "      <td>2.148</td>\n",
       "      <td>290.547</td>\n",
       "      <td>3</td>\n",
       "      <td>76.773</td>\n",
       "      <td>-0.790</td>\n",
       "      <td>0.303</td>\n",
       "      <td>56.876</td>\n",
       "      <td>1.773</td>\n",
       "      <td>-2.079</td>\n",
       "      <td>165.640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  \\\n",
       "0   100000          s       138.470                       51.655   \n",
       "1   100001          b       160.937                       68.768   \n",
       "2   100002          b      -999.000                      162.172   \n",
       "3   100003          b       143.905                       81.417   \n",
       "4   100004          b       175.864                       16.915   \n",
       "5   100005          b        89.744                       13.550   \n",
       "6   100006          s       148.754                       28.862   \n",
       "7   100007          s       154.916                       10.418   \n",
       "8   100008          b       105.594                       50.559   \n",
       "9   100009          s       128.053                       88.941   \n",
       "10  100010          b      -999.000                       86.240   \n",
       "11  100011          b       114.744                       10.286   \n",
       "\n",
       "    DER_mass_vis  DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  \\\n",
       "0         97.827    27.980                 0.910           124.711   \n",
       "1        103.235    48.146              -999.000          -999.000   \n",
       "2        125.953    35.635              -999.000          -999.000   \n",
       "3         80.943     0.414              -999.000          -999.000   \n",
       "4        134.805    16.405              -999.000          -999.000   \n",
       "5         59.149   116.344                 2.636           284.584   \n",
       "6        107.782   106.130                 0.733           158.359   \n",
       "7         94.714    29.169              -999.000          -999.000   \n",
       "8        100.989     4.288              -999.000          -999.000   \n",
       "9         69.272   193.392              -999.000          -999.000   \n",
       "10        79.692    27.201              -999.000          -999.000   \n",
       "11        75.712    30.816                 2.563           252.599   \n",
       "\n",
       "    DER_prodeta_jet_jet  DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  \\\n",
       "0                 2.666               3.064      41.928     197.760   \n",
       "1              -999.000               3.473       2.078     125.157   \n",
       "2              -999.000               3.148       9.336     197.814   \n",
       "3              -999.000               3.310       0.414      75.968   \n",
       "4              -999.000               3.891      16.405      57.983   \n",
       "5                -0.540               1.362      61.619     278.876   \n",
       "6                 0.113               2.941       2.545     305.967   \n",
       "7              -999.000               2.897       1.526     138.178   \n",
       "8              -999.000               2.904       4.288      65.333   \n",
       "9              -999.000               1.609      28.859     255.123   \n",
       "10             -999.000               2.338      27.201      81.734   \n",
       "11               -1.401               2.888      36.745     239.804   \n",
       "\n",
       "    DER_pt_ratio_lep_tau  DER_met_phi_centrality  DER_lep_eta_centrality  \\\n",
       "0                  1.582                   1.396                   0.200   \n",
       "1                  0.879                   1.414                -999.000   \n",
       "2                  3.776                   1.414                -999.000   \n",
       "3                  2.354                  -1.285                -999.000   \n",
       "4                  1.056                  -1.385                -999.000   \n",
       "5                  0.588                   0.479                   0.975   \n",
       "6                  3.371                   1.393                   0.791   \n",
       "7                  0.365                  -1.305                -999.000   \n",
       "8                  0.675                  -1.366                -999.000   \n",
       "9                  0.599                   0.538                -999.000   \n",
       "10                 1.750                  -1.412                -999.000   \n",
       "11                 1.061                   1.364                   0.769   \n",
       "\n",
       "    PRI_tau_pt  PRI_tau_eta  PRI_tau_phi  PRI_lep_pt  PRI_lep_eta  \\\n",
       "0       32.638        1.017        0.381      51.626        2.273   \n",
       "1       42.014        2.039       -3.011      36.918        0.501   \n",
       "2       32.154       -0.705       -2.093     121.409       -0.953   \n",
       "3       22.647       -1.655        0.010      53.321       -0.522   \n",
       "4       28.209       -2.197       -2.231      29.774        0.798   \n",
       "5       53.651        0.371        1.329      31.565       -0.884   \n",
       "6       28.850        1.113        2.409      97.240        0.675   \n",
       "7       78.800        0.654        1.547      28.740        0.506   \n",
       "8       39.008        2.433       -2.532      26.325        0.210   \n",
       "9       54.646       -1.533        0.416      32.742       -0.317   \n",
       "10      29.718       -0.866        2.878      52.016        0.126   \n",
       "11      35.976       -0.669       -0.342      38.188       -0.165   \n",
       "\n",
       "    PRI_lep_phi  PRI_met  PRI_met_phi  PRI_met_sumet  PRI_jet_num  \\\n",
       "0        -2.414   16.824       -0.277        258.733            2   \n",
       "1         0.103   44.704       -1.916        164.546            1   \n",
       "2         1.052   54.283       -2.186        260.414            1   \n",
       "3        -3.100   31.082        0.060         86.062            0   \n",
       "4         1.569    2.723       -0.871         53.131            0   \n",
       "5         1.857   40.735        2.237        282.849            3   \n",
       "6        -0.966   38.421       -1.443        294.074            2   \n",
       "7        -1.347   22.275       -1.761        187.299            1   \n",
       "8         1.884   37.791        0.024        129.804            0   \n",
       "9        -0.636  132.678        0.845        294.741            1   \n",
       "10       -1.288   51.276        0.688        250.178            0   \n",
       "11        2.502   22.385        2.148        290.547            3   \n",
       "\n",
       "    PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n",
       "0               67.435                2.150                0.444   \n",
       "1               46.226                0.725                1.158   \n",
       "2               44.251                2.053               -2.028   \n",
       "3             -999.000             -999.000             -999.000   \n",
       "4             -999.000             -999.000             -999.000   \n",
       "5               90.547               -2.412               -0.653   \n",
       "6              123.010                0.864                1.450   \n",
       "7               30.638               -0.715               -1.724   \n",
       "8             -999.000             -999.000             -999.000   \n",
       "9              167.735               -2.767               -2.514   \n",
       "10            -999.000             -999.000             -999.000   \n",
       "11              76.773               -0.790                0.303   \n",
       "\n",
       "    PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n",
       "0                  46.062                   1.240                  -2.475   \n",
       "1                -999.000                -999.000                -999.000   \n",
       "2                -999.000                -999.000                -999.000   \n",
       "3                -999.000                -999.000                -999.000   \n",
       "4                -999.000                -999.000                -999.000   \n",
       "5                  56.165                   0.224                   3.106   \n",
       "6                  56.867                   0.131                  -2.767   \n",
       "7                -999.000                -999.000                -999.000   \n",
       "8                -999.000                -999.000                -999.000   \n",
       "9                -999.000                -999.000                -999.000   \n",
       "10               -999.000                -999.000                -999.000   \n",
       "11                 56.876                   1.773                  -2.079   \n",
       "\n",
       "    PRI_jet_all_pt  \n",
       "0          113.497  \n",
       "1           46.226  \n",
       "2           44.251  \n",
       "3            0.000  \n",
       "4            0.000  \n",
       "5          193.660  \n",
       "6          179.877  \n",
       "7           30.638  \n",
       "8            0.000  \n",
       "9          167.735  \n",
       "10           0.000  \n",
       "11         165.640  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(DATA_TRAIN_PATH)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data tX:  (250000, 30)\n",
      "**\n",
      "250000\n",
      "[ 138.47   160.937 -999.     143.905  175.864   89.744  148.754  154.916\n",
      "  105.594  128.053 -999.     114.744  145.297   82.488 -999.     111.026\n",
      "  114.256  127.861 -999.    -999.    -999.      90.736   87.075  141.481\n",
      "  110.785   76.883  137.197  111.271  118.104   98.761]\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of data tX: \",tX.shape)\n",
    "print(\"**\")\n",
    "print(len(tX[:,1]))\n",
    "\n",
    "print(tX[0:30,0])\n",
    "print(\"**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 features removed (features number:  [ 4  5  6 12 26 27 28] )\n",
      "new shape of data: (250000, 23)\n"
     ]
    }
   ],
   "source": [
    "y[np.nonzero(y == -1)] = 0\n",
    "Data = remove_features_with_too_many_missing_values(tX,0.66)\n",
    "Data = replace_missing_values_with_global_mean(Data)\n",
    "ZData = Z_score_of_each_feature(Data)\n",
    "\n",
    "trainx,trainy,ids_test, validationx,validationy, idstest = split_data(ZData, y, ids, 0.75, seed=1)\n",
    "#trainy = np.transpose(np.array(trainy,ndmin=2))\n",
    "\n",
    "trainx = np.delete(trainx, [2,3,6,7,17,22], axis=1)\n",
    "validationx = np.delete(validationx, [2,3,6,7,17,22], axis=1)\n",
    "trainyas = np.transpose(np.array(trainy,ndmin=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.149107e-01</td>\n",
       "      <td>0.068332</td>\n",
       "      <td>0.407680</td>\n",
       "      <td>-0.469966</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>1.033099</td>\n",
       "      <td>0.339894</td>\n",
       "      <td>0.170929</td>\n",
       "      <td>1.277084</td>\n",
       "      <td>-0.270811</td>\n",
       "      <td>0.846712</td>\n",
       "      <td>0.214212</td>\n",
       "      <td>0.225054</td>\n",
       "      <td>1.812288</td>\n",
       "      <td>-1.352820</td>\n",
       "      <td>-0.756757</td>\n",
       "      <td>-0.147267</td>\n",
       "      <td>0.386847</td>\n",
       "      <td>1.044402</td>\n",
       "      <td>-0.369921</td>\n",
       "      <td>1.557298e+00</td>\n",
       "      <td>3.248244e-01</td>\n",
       "      <td>0.412510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.408270e-01</td>\n",
       "      <td>0.552505</td>\n",
       "      <td>0.540136</td>\n",
       "      <td>-0.153167</td>\n",
       "      <td>1.404888</td>\n",
       "      <td>-0.756027</td>\n",
       "      <td>-0.287584</td>\n",
       "      <td>-0.661279</td>\n",
       "      <td>1.292164</td>\n",
       "      <td>0.147536</td>\n",
       "      <td>1.688504</td>\n",
       "      <td>-1.652849</td>\n",
       "      <td>-0.441526</td>\n",
       "      <td>0.411475</td>\n",
       "      <td>0.032730</td>\n",
       "      <td>0.090798</td>\n",
       "      <td>-1.051683</td>\n",
       "      <td>-0.357719</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.821154</td>\n",
       "      <td>5.267049e-01</td>\n",
       "      <td>8.329932e-01</td>\n",
       "      <td>-0.273820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.388023e-16</td>\n",
       "      <td>3.195156</td>\n",
       "      <td>1.096560</td>\n",
       "      <td>-0.349710</td>\n",
       "      <td>0.989770</td>\n",
       "      <td>-0.430168</td>\n",
       "      <td>0.340361</td>\n",
       "      <td>2.768174</td>\n",
       "      <td>1.292164</td>\n",
       "      <td>-0.292406</td>\n",
       "      <td>-0.571650</td>\n",
       "      <td>-1.147554</td>\n",
       "      <td>3.387682</td>\n",
       "      <td>-0.737951</td>\n",
       "      <td>0.555132</td>\n",
       "      <td>0.382001</td>\n",
       "      <td>-1.200672</td>\n",
       "      <td>0.400135</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-0.863173</td>\n",
       "      <td>1.487145e+00</td>\n",
       "      <td>-1.434550e+00</td>\n",
       "      <td>-0.293970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.179442e-01</td>\n",
       "      <td>0.910379</td>\n",
       "      <td>-0.005853</td>\n",
       "      <td>-0.903016</td>\n",
       "      <td>1.196690</td>\n",
       "      <td>-0.830735</td>\n",
       "      <td>-0.712705</td>\n",
       "      <td>1.084818</td>\n",
       "      <td>-0.969095</td>\n",
       "      <td>-0.716598</td>\n",
       "      <td>-1.354138</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.301873</td>\n",
       "      <td>-0.397234</td>\n",
       "      <td>-1.730447</td>\n",
       "      <td>-0.323312</td>\n",
       "      <td>0.038692</td>\n",
       "      <td>-0.978149</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.003673e-17</td>\n",
       "      <td>1.111175e-17</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.023804e+00</td>\n",
       "      <td>-0.914556</td>\n",
       "      <td>1.313369</td>\n",
       "      <td>-0.651804</td>\n",
       "      <td>1.938794</td>\n",
       "      <td>-0.112795</td>\n",
       "      <td>-0.868143</td>\n",
       "      <td>-0.451747</td>\n",
       "      <td>-1.052877</td>\n",
       "      <td>-0.468428</td>\n",
       "      <td>-1.800568</td>\n",
       "      <td>-1.223513</td>\n",
       "      <td>-0.765298</td>\n",
       "      <td>0.646261</td>\n",
       "      <td>0.839728</td>\n",
       "      <td>-1.185429</td>\n",
       "      <td>-0.475042</td>\n",
       "      <td>-1.238475</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.003673e-17</td>\n",
       "      <td>1.111175e-17</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-6.088086e-01</td>\n",
       "      <td>-1.009761</td>\n",
       "      <td>-0.539646</td>\n",
       "      <td>0.918192</td>\n",
       "      <td>-1.291464</td>\n",
       "      <td>1.917156</td>\n",
       "      <td>1.040948</td>\n",
       "      <td>-1.005763</td>\n",
       "      <td>0.508808</td>\n",
       "      <td>0.666766</td>\n",
       "      <td>0.314620</td>\n",
       "      <td>0.736020</td>\n",
       "      <td>-0.684128</td>\n",
       "      <td>-0.683404</td>\n",
       "      <td>0.998266</td>\n",
       "      <td>-0.029860</td>\n",
       "      <td>1.239982</td>\n",
       "      <td>0.577488</td>\n",
       "      <td>2.067499</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>-1.742045e+00</td>\n",
       "      <td>-4.559336e-01</td>\n",
       "      <td>1.230371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.098688e-01</td>\n",
       "      <td>-0.576543</td>\n",
       "      <td>0.651504</td>\n",
       "      <td>0.757735</td>\n",
       "      <td>0.725371</td>\n",
       "      <td>-0.735060</td>\n",
       "      <td>1.275085</td>\n",
       "      <td>2.288737</td>\n",
       "      <td>1.274570</td>\n",
       "      <td>-0.439827</td>\n",
       "      <td>0.925785</td>\n",
       "      <td>1.330485</td>\n",
       "      <td>2.292321</td>\n",
       "      <td>0.549027</td>\n",
       "      <td>-0.555730</td>\n",
       "      <td>-0.100206</td>\n",
       "      <td>-0.790677</td>\n",
       "      <td>0.666224</td>\n",
       "      <td>1.044402</td>\n",
       "      <td>0.812469</td>\n",
       "      <td>6.272329e-01</td>\n",
       "      <td>1.040816e+00</td>\n",
       "      <td>1.089751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.266844e-01</td>\n",
       "      <td>-1.098374</td>\n",
       "      <td>0.331435</td>\n",
       "      <td>-0.451288</td>\n",
       "      <td>0.669171</td>\n",
       "      <td>-0.780810</td>\n",
       "      <td>-0.175049</td>\n",
       "      <td>-1.269749</td>\n",
       "      <td>-0.985852</td>\n",
       "      <td>1.788886</td>\n",
       "      <td>0.547719</td>\n",
       "      <td>0.856014</td>\n",
       "      <td>-0.812160</td>\n",
       "      <td>0.415428</td>\n",
       "      <td>-0.765461</td>\n",
       "      <td>-0.591046</td>\n",
       "      <td>-0.966153</td>\n",
       "      <td>-0.177852</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>-1.152798</td>\n",
       "      <td>-5.147361e-01</td>\n",
       "      <td>-1.218187e+00</td>\n",
       "      <td>-0.432856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.083335e-01</td>\n",
       "      <td>0.037323</td>\n",
       "      <td>0.485126</td>\n",
       "      <td>-0.842157</td>\n",
       "      <td>0.678112</td>\n",
       "      <td>-0.656806</td>\n",
       "      <td>-0.804620</td>\n",
       "      <td>-0.902773</td>\n",
       "      <td>-1.036958</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>2.013031</td>\n",
       "      <td>-1.389193</td>\n",
       "      <td>-0.921610</td>\n",
       "      <td>0.181432</td>\n",
       "      <td>1.013129</td>\n",
       "      <td>-0.119358</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>-0.632361</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.003673e-17</td>\n",
       "      <td>1.111175e-17</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.174312e-01</td>\n",
       "      <td>1.123253</td>\n",
       "      <td>-0.291707</td>\n",
       "      <td>2.128582</td>\n",
       "      <td>-0.975975</td>\n",
       "      <td>0.446346</td>\n",
       "      <td>0.835660</td>\n",
       "      <td>-0.992741</td>\n",
       "      <td>0.558239</td>\n",
       "      <td>0.711162</td>\n",
       "      <td>-1.253650</td>\n",
       "      <td>0.233477</td>\n",
       "      <td>-0.630786</td>\n",
       "      <td>-0.235176</td>\n",
       "      <td>-0.374073</td>\n",
       "      <td>2.765216</td>\n",
       "      <td>0.471863</td>\n",
       "      <td>0.671497</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>1.764019</td>\n",
       "      <td>-1.998790e+00</td>\n",
       "      <td>-1.780446e+00</td>\n",
       "      <td>0.965872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-5.388023e-16</td>\n",
       "      <td>1.046835</td>\n",
       "      <td>-0.036494</td>\n",
       "      <td>-0.482204</td>\n",
       "      <td>-0.044833</td>\n",
       "      <td>0.371908</td>\n",
       "      <td>-0.662872</td>\n",
       "      <td>0.369806</td>\n",
       "      <td>-1.075498</td>\n",
       "      <td>-0.401098</td>\n",
       "      <td>-0.704261</td>\n",
       "      <td>1.588637</td>\n",
       "      <td>0.242729</td>\n",
       "      <td>0.115028</td>\n",
       "      <td>-0.732983</td>\n",
       "      <td>0.290587</td>\n",
       "      <td>0.385229</td>\n",
       "      <td>0.319218</td>\n",
       "      <td>-1.001792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.003673e-17</td>\n",
       "      <td>1.111175e-17</td>\n",
       "      <td>-0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.348731e-01</td>\n",
       "      <td>-1.102108</td>\n",
       "      <td>-0.133974</td>\n",
       "      <td>-0.425414</td>\n",
       "      <td>0.657675</td>\n",
       "      <td>0.800400</td>\n",
       "      <td>0.703264</td>\n",
       "      <td>-0.445828</td>\n",
       "      <td>1.250274</td>\n",
       "      <td>-0.121873</td>\n",
       "      <td>-0.541998</td>\n",
       "      <td>-0.183750</td>\n",
       "      <td>-0.383968</td>\n",
       "      <td>-0.115016</td>\n",
       "      <td>1.353323</td>\n",
       "      <td>-0.587702</td>\n",
       "      <td>1.190871</td>\n",
       "      <td>0.638342</td>\n",
       "      <td>2.067499</td>\n",
       "      <td>-0.171249</td>\n",
       "      <td>-5.689778e-01</td>\n",
       "      <td>2.244717e-01</td>\n",
       "      <td>0.944498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0   3.149107e-01  0.068332  0.407680 -0.469966  0.882478  1.033099  0.339894   \n",
       "1   7.408270e-01  0.552505  0.540136 -0.153167  1.404888 -0.756027 -0.287584   \n",
       "2  -5.388023e-16  3.195156  1.096560 -0.349710  0.989770 -0.430168  0.340361   \n",
       "3   4.179442e-01  0.910379 -0.005853 -0.903016  1.196690 -0.830735 -0.712705   \n",
       "4   1.023804e+00 -0.914556  1.313369 -0.651804  1.938794 -0.112795 -0.868143   \n",
       "5  -6.088086e-01 -1.009761 -0.539646  0.918192 -1.291464  1.917156  1.040948   \n",
       "6   5.098688e-01 -0.576543  0.651504  0.757735  0.725371 -0.735060  1.275085   \n",
       "7   6.266844e-01 -1.098374  0.331435 -0.451288  0.669171 -0.780810 -0.175049   \n",
       "8  -3.083335e-01  0.037323  0.485126 -0.842157  0.678112 -0.656806 -0.804620   \n",
       "9   1.174312e-01  1.123253 -0.291707  2.128582 -0.975975  0.446346  0.835660   \n",
       "10 -5.388023e-16  1.046835 -0.036494 -0.482204 -0.044833  0.371908 -0.662872   \n",
       "11 -1.348731e-01 -1.102108 -0.133974 -0.425414  0.657675  0.800400  0.703264   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.170929  1.277084 -0.270811  0.846712  0.214212  0.225054  1.812288   \n",
       "1  -0.661279  1.292164  0.147536  1.688504 -1.652849 -0.441526  0.411475   \n",
       "2   2.768174  1.292164 -0.292406 -0.571650 -1.147554  3.387682 -0.737951   \n",
       "3   1.084818 -0.969095 -0.716598 -1.354138  0.010002  0.301873 -0.397234   \n",
       "4  -0.451747 -1.052877 -0.468428 -1.800568 -1.223513 -0.765298  0.646261   \n",
       "5  -1.005763  0.508808  0.666766  0.314620  0.736020 -0.684128 -0.683404   \n",
       "6   2.288737  1.274570 -0.439827  0.925785  1.330485  2.292321  0.549027   \n",
       "7  -1.269749 -0.985852  1.788886  0.547719  0.856014 -0.812160  0.415428   \n",
       "8  -0.902773 -1.036958  0.013412  2.013031 -1.389193 -0.921610  0.181432   \n",
       "9  -0.992741  0.558239  0.711162 -1.253650  0.233477 -0.630786 -0.235176   \n",
       "10  0.369806 -1.075498 -0.401098 -0.704261  1.588637  0.242729  0.115028   \n",
       "11 -0.445828  1.250274 -0.121873 -0.541998 -0.183750 -0.383968 -0.115016   \n",
       "\n",
       "          14        15        16        17        18        19            20  \\\n",
       "0  -1.352820 -0.756757 -0.147267  0.386847  1.044402 -0.369921  1.557298e+00   \n",
       "1   0.032730  0.090798 -1.051683 -0.357719  0.021305 -0.821154  5.267049e-01   \n",
       "2   0.555132  0.382001 -1.200672  0.400135  0.021305 -0.863173  1.487145e+00   \n",
       "3  -1.730447 -0.323312  0.038692 -0.978149 -1.001792  0.000000  1.003673e-17   \n",
       "4   0.839728 -1.185429 -0.475042 -1.238475 -1.001792  0.000000  1.003673e-17   \n",
       "5   0.998266 -0.029860  1.239982  0.577488  2.067499  0.121800 -1.742045e+00   \n",
       "6  -0.555730 -0.100206 -0.790677  0.666224  1.044402  0.812469  6.272329e-01   \n",
       "7  -0.765461 -0.591046 -0.966153 -0.177852  0.021305 -1.152798 -5.147361e-01   \n",
       "8   1.013129 -0.119358  0.018827 -0.632361 -1.001792  0.000000  1.003673e-17   \n",
       "9  -0.374073  2.765216  0.471863  0.671497  0.021305  1.764019 -1.998790e+00   \n",
       "10 -0.732983  0.290587  0.385229  0.319218 -1.001792  0.000000  1.003673e-17   \n",
       "11  1.353323 -0.587702  1.190871  0.638342  2.067499 -0.171249 -5.689778e-01   \n",
       "\n",
       "              21        22  \n",
       "0   3.248244e-01  0.412510  \n",
       "1   8.329932e-01 -0.273820  \n",
       "2  -1.434550e+00 -0.293970  \n",
       "3   1.111175e-17 -0.745439  \n",
       "4   1.111175e-17 -0.745439  \n",
       "5  -4.559336e-01  1.230371  \n",
       "6   1.040816e+00  1.089751  \n",
       "7  -1.218187e+00 -0.432856  \n",
       "8   1.111175e-17 -0.745439  \n",
       "9  -1.780446e+00  0.965872  \n",
       "10  1.111175e-17 -0.745439  \n",
       "11  2.244717e-01  0.944498  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualization\n",
    "Data_df = pd.DataFrame(ZData)\n",
    "Data_df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signaldata = ZData[np.nonzero(y == 1)]\n",
    "nosignaldata = ZData[np.nonzero(y == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signaldata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-afb9ba2f8a25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignaldata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnosignaldata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'signaldata' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(ZData.shape[1]):\n",
    "    plt.figure\n",
    "    plt.hist(signaldata[:,i],100,color='b')\n",
    "    plt.hist(nosignaldata[:,i],100,color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x16cdfa00898>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cov = np.corrcoef(np.transpose(trainx))\n",
    "plt.imshow(cov)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599453\n",
      "Current iteration=10, loss=0.689085508708236\n",
      "Current iteration=20, loss=0.6846890439844013\n",
      "Current iteration=30, loss=0.6835610272810004\n",
      "Current iteration=40, loss=0.6770263749082273\n",
      "Current iteration=50, loss=0.6769446649993718\n",
      "Current iteration=60, loss=0.6761885485969354\n",
      "Current iteration=70, loss=0.668541914757744\n",
      "Current iteration=80, loss=0.677976450047582\n",
      "Current iteration=90, loss=0.6492290953943263\n",
      "Current iteration=100, loss=0.6455552665296027\n",
      "Current iteration=110, loss=0.6439672440776376\n",
      "Current iteration=120, loss=0.655278898545945\n",
      "Current iteration=130, loss=0.6406584297127721\n",
      "Current iteration=140, loss=0.6318900589081102\n",
      "Current iteration=150, loss=0.6417279639962519\n",
      "Current iteration=160, loss=0.6327204081275659\n",
      "Current iteration=170, loss=0.6353685989592555\n",
      "Current iteration=180, loss=0.642536771801237\n",
      "Current iteration=190, loss=0.6298691309693251\n",
      "Current iteration=200, loss=0.6280387580016145\n",
      "Current iteration=210, loss=0.6161151266512451\n",
      "Current iteration=220, loss=0.6569116881690367\n",
      "Current iteration=230, loss=0.6267397323278339\n",
      "Current iteration=240, loss=0.6351235983078916\n",
      "Current iteration=250, loss=0.6256396127519676\n",
      "Current iteration=260, loss=0.6113545280717192\n",
      "Current iteration=270, loss=0.5915658036096797\n",
      "Current iteration=280, loss=0.6119544596475842\n",
      "Current iteration=290, loss=0.594660457388275\n",
      "Current iteration=300, loss=0.59955989791926\n",
      "Current iteration=310, loss=0.611995148989213\n",
      "Current iteration=320, loss=0.5885061280534409\n",
      "Current iteration=330, loss=0.6126644584223963\n",
      "Current iteration=340, loss=0.5933954040487247\n",
      "Current iteration=350, loss=0.5921255139637659\n",
      "Current iteration=360, loss=0.5757187336560211\n",
      "Current iteration=370, loss=0.6017000112417394\n",
      "Current iteration=380, loss=0.5885581317207292\n",
      "Current iteration=390, loss=0.5686369353845285\n",
      "Current iteration=400, loss=0.5877724761782889\n",
      "Current iteration=410, loss=0.5799643778672458\n",
      "Current iteration=420, loss=0.5741228934278481\n",
      "Current iteration=430, loss=0.5586685264932627\n",
      "Current iteration=440, loss=0.5865191603903577\n",
      "Current iteration=450, loss=0.5743615474973499\n",
      "Current iteration=460, loss=0.5920534461599413\n",
      "Current iteration=470, loss=0.5692666832897775\n",
      "Current iteration=480, loss=0.564436998897587\n",
      "Current iteration=490, loss=0.5855934890893127\n",
      "Current iteration=500, loss=0.5476974660169349\n",
      "Current iteration=510, loss=0.5191367987615367\n",
      "Current iteration=520, loss=0.6116361317900023\n",
      "Current iteration=530, loss=0.5751915665828218\n",
      "Current iteration=540, loss=0.5595929872790424\n",
      "Current iteration=550, loss=0.5676964558166232\n",
      "Current iteration=560, loss=0.6004375487798537\n",
      "Current iteration=570, loss=0.5900322225159735\n",
      "Current iteration=580, loss=0.5553529379227927\n",
      "Current iteration=590, loss=0.5593436892747097\n",
      "Current iteration=600, loss=0.5596867447235429\n",
      "Current iteration=610, loss=0.5738269038346442\n",
      "Current iteration=620, loss=0.5384097640775418\n",
      "Current iteration=630, loss=0.547754093855459\n",
      "Current iteration=640, loss=0.5682117702998477\n",
      "Current iteration=650, loss=0.5513639132021779\n",
      "Current iteration=660, loss=0.5300253909257657\n",
      "Current iteration=670, loss=0.5559852864684904\n",
      "Current iteration=680, loss=0.5517741793771741\n",
      "Current iteration=690, loss=0.5413927894742065\n",
      "Current iteration=700, loss=0.5974292474408176\n",
      "Current iteration=710, loss=0.49730005700409996\n",
      "Current iteration=720, loss=0.5604119545941589\n",
      "Current iteration=730, loss=0.5310282737079899\n",
      "Current iteration=740, loss=0.5160893038978418\n",
      "Current iteration=750, loss=0.5925789600715798\n",
      "Current iteration=760, loss=0.4922055951898136\n",
      "Current iteration=770, loss=0.5151283190721372\n",
      "Current iteration=780, loss=0.5350037808828063\n",
      "Current iteration=790, loss=0.5089764322592703\n",
      "Current iteration=800, loss=0.5537263979829146\n",
      "Current iteration=810, loss=0.5526732737490075\n",
      "Current iteration=820, loss=0.5149043034777108\n",
      "Current iteration=830, loss=0.5141762074013285\n",
      "Current iteration=840, loss=0.5576035902154866\n",
      "Current iteration=850, loss=0.5211647838963478\n",
      "Current iteration=860, loss=0.5463197197471987\n",
      "Current iteration=870, loss=0.5172987500293779\n",
      "Current iteration=880, loss=0.5266750327703922\n",
      "Current iteration=890, loss=0.5275118704936476\n",
      "Current iteration=900, loss=0.49875873323100417\n",
      "Current iteration=910, loss=0.508765396336036\n",
      "Current iteration=920, loss=0.536717739584114\n",
      "Current iteration=930, loss=0.5381585953937994\n",
      "Current iteration=940, loss=0.5445754516556431\n",
      "Current iteration=950, loss=0.5638816129059571\n",
      "Current iteration=960, loss=0.5320556889756829\n",
      "Current iteration=970, loss=0.516820430848528\n",
      "Current iteration=980, loss=0.4843956975430639\n",
      "Current iteration=990, loss=0.5204186703419851\n",
      "Current iteration=1000, loss=0.5610318698721019\n",
      "Current iteration=1010, loss=0.5450445512696751\n",
      "Current iteration=1020, loss=0.47404995439742237\n",
      "Current iteration=1030, loss=0.5100053714351791\n",
      "Current iteration=1040, loss=0.5465054797871416\n",
      "Current iteration=1050, loss=0.5090525939060592\n",
      "Current iteration=1060, loss=0.4983164949661344\n",
      "Current iteration=1070, loss=0.5072191111323455\n",
      "Current iteration=1080, loss=0.4869673551553055\n",
      "Current iteration=1090, loss=0.524973979957593\n",
      "Current iteration=1100, loss=0.5255610198695906\n",
      "Current iteration=1110, loss=0.5017576381869783\n",
      "Current iteration=1120, loss=0.5138689492399084\n",
      "Current iteration=1130, loss=0.5622564398249924\n",
      "Current iteration=1140, loss=0.5738417856587654\n",
      "Current iteration=1150, loss=0.5311505978988711\n",
      "Current iteration=1160, loss=0.5222453198413796\n",
      "Current iteration=1170, loss=0.4485656173917367\n",
      "Current iteration=1180, loss=0.46201022334043346\n",
      "Current iteration=1190, loss=0.48584382346451993\n",
      "Current iteration=1200, loss=0.49220782856191325\n",
      "Current iteration=1210, loss=0.5263266954839895\n",
      "Current iteration=1220, loss=0.5102047547646484\n",
      "Current iteration=1230, loss=0.5011788416928676\n",
      "Current iteration=1240, loss=0.5173926146148636\n",
      "Current iteration=1250, loss=0.5091787030914027\n",
      "Current iteration=1260, loss=0.49372479327749874\n",
      "Current iteration=1270, loss=0.534468453460395\n",
      "Current iteration=1280, loss=0.5294706392568684\n",
      "Current iteration=1290, loss=0.4608316095202373\n",
      "Current iteration=1300, loss=0.5133556673236752\n",
      "Current iteration=1310, loss=0.4533984498824594\n",
      "Current iteration=1320, loss=0.4618495244597131\n",
      "Current iteration=1330, loss=0.532064788607791\n",
      "Current iteration=1340, loss=0.44256487668116684\n",
      "Current iteration=1350, loss=0.4673557163950736\n",
      "Current iteration=1360, loss=0.4967980221848808\n",
      "Current iteration=1370, loss=0.47730702086505944\n",
      "Current iteration=1380, loss=0.4611268790967522\n",
      "Current iteration=1390, loss=0.5294664669796808\n",
      "Current iteration=1400, loss=0.45190369842218125\n",
      "Current iteration=1410, loss=0.47225702273679937\n",
      "Current iteration=1420, loss=0.519563699659331\n",
      "Current iteration=1430, loss=0.5086682640059567\n",
      "Current iteration=1440, loss=0.4690050329217598\n",
      "Current iteration=1450, loss=0.45882636790072234\n",
      "Current iteration=1460, loss=0.44350166574163696\n",
      "Current iteration=1470, loss=0.49093697280101767\n",
      "Current iteration=1480, loss=0.43914955183008686\n",
      "Current iteration=1490, loss=0.44347383122119055\n",
      "Current iteration=1500, loss=0.5070668032958137\n",
      "Current iteration=1510, loss=0.4423316097987865\n",
      "Current iteration=1520, loss=0.4327387902721905\n",
      "Current iteration=1530, loss=0.44840178845627576\n",
      "Current iteration=1540, loss=0.5114543996316268\n",
      "Current iteration=1550, loss=0.5183276615481592\n",
      "Current iteration=1560, loss=0.444314661185227\n",
      "Current iteration=1570, loss=0.4729475241963468\n",
      "Current iteration=1580, loss=0.46381549849804476\n",
      "Current iteration=1590, loss=0.418170585146135\n",
      "Current iteration=1600, loss=0.4982857088496298\n",
      "Current iteration=1610, loss=0.45709165887019\n",
      "Current iteration=1620, loss=0.45960857364404445\n",
      "Current iteration=1630, loss=0.45904889864549075\n",
      "Current iteration=1640, loss=0.46196380000821086\n",
      "Current iteration=1650, loss=0.4834814920704939\n",
      "Current iteration=1660, loss=0.5119239502822446\n",
      "Current iteration=1670, loss=0.517849649002532\n",
      "Current iteration=1680, loss=0.5047210605627561\n",
      "Current iteration=1690, loss=0.49734581846633275\n",
      "Current iteration=1700, loss=0.5205677237969724\n",
      "Current iteration=1710, loss=0.5062742740573843\n",
      "Current iteration=1720, loss=0.48797724985898844\n",
      "Current iteration=1730, loss=0.49246663879773683\n",
      "Current iteration=1740, loss=0.515994933639442\n",
      "Current iteration=1750, loss=0.48803176413765265\n",
      "Current iteration=1760, loss=0.43176757910326274\n",
      "Current iteration=1770, loss=0.508512555657448\n",
      "Current iteration=1780, loss=0.4633500103889269\n",
      "Current iteration=1790, loss=0.4991915154221694\n",
      "Current iteration=1800, loss=0.4872623327944568\n",
      "Current iteration=1810, loss=0.4179593584897116\n",
      "Current iteration=1820, loss=0.47475708499659214\n",
      "Current iteration=1830, loss=0.43419586552194744\n",
      "Current iteration=1840, loss=0.44787187317245036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1850, loss=0.480998280979053\n",
      "Current iteration=1860, loss=0.5003212058896631\n",
      "Current iteration=1870, loss=0.4641390960697476\n",
      "Current iteration=1880, loss=0.5274138012811733\n",
      "Current iteration=1890, loss=0.5574362299688291\n",
      "Current iteration=1900, loss=0.5669433134198187\n",
      "Current iteration=1910, loss=0.41803960495585807\n",
      "Current iteration=1920, loss=0.733929980615442\n",
      "Current iteration=1930, loss=0.4632317464650679\n",
      "Current iteration=1940, loss=0.5131020898848657\n",
      "Current iteration=1950, loss=0.4596779165246817\n",
      "Current iteration=1960, loss=0.556206124823228\n",
      "Current iteration=1970, loss=0.4908230735725777\n",
      "Current iteration=1980, loss=0.4536412100918992\n",
      "Current iteration=1990, loss=0.507416231255213\n",
      "Current iteration=2000, loss=0.44731130945967623\n",
      "Current iteration=2010, loss=0.40313174615814795\n",
      "Current iteration=2020, loss=0.47464242727610406\n",
      "Current iteration=2030, loss=0.46100947853701946\n",
      "Current iteration=2040, loss=0.4829627029705666\n",
      "Current iteration=2050, loss=0.46252145246545384\n",
      "Current iteration=2060, loss=0.43763990738925124\n",
      "Current iteration=2070, loss=0.5105304645264117\n",
      "Current iteration=2080, loss=0.42654597431059016\n",
      "Current iteration=2090, loss=0.4783577203203484\n",
      "Current iteration=2100, loss=0.4002611502193201\n",
      "Current iteration=2110, loss=0.42708519158166747\n",
      "Current iteration=2120, loss=0.41074586387784634\n",
      "Current iteration=2130, loss=0.3996983162897849\n",
      "Current iteration=2140, loss=0.4565197591488812\n",
      "Current iteration=2150, loss=0.5398707348521941\n",
      "Current iteration=2160, loss=0.42744538035455376\n",
      "Current iteration=2170, loss=0.4703079605442317\n",
      "Current iteration=2180, loss=0.4089928803602057\n",
      "Current iteration=2190, loss=0.5411095961075261\n",
      "Current iteration=2200, loss=0.5223468970152997\n",
      "Current iteration=2210, loss=0.4623457280955015\n",
      "Current iteration=2220, loss=0.47766949643615064\n",
      "Current iteration=2230, loss=0.4484702967228044\n",
      "Current iteration=2240, loss=0.4113244045531415\n",
      "Current iteration=2250, loss=0.4801359057677498\n",
      "Current iteration=2260, loss=0.48921198477285854\n",
      "Current iteration=2270, loss=0.4782748037141183\n",
      "Current iteration=2280, loss=0.393342692415943\n",
      "Current iteration=2290, loss=0.44908120918006406\n",
      "Current iteration=2300, loss=0.5389871162297752\n",
      "Current iteration=2310, loss=0.4961456702553235\n",
      "Current iteration=2320, loss=0.44560534052825296\n",
      "Current iteration=2330, loss=0.4378641861278181\n",
      "Current iteration=2340, loss=0.47676084639340205\n",
      "Current iteration=2350, loss=0.5357443387345343\n",
      "Current iteration=2360, loss=0.47116315209914406\n",
      "Current iteration=2370, loss=0.42573131149875304\n",
      "Current iteration=2380, loss=0.4999385270789504\n",
      "Current iteration=2390, loss=0.3818601491722543\n",
      "Current iteration=2400, loss=0.46587821098197474\n",
      "Current iteration=2410, loss=0.5439412142899979\n",
      "Current iteration=2420, loss=0.4456346462085552\n",
      "Current iteration=2430, loss=0.4373781538600627\n",
      "Current iteration=2440, loss=0.4700525857063573\n",
      "Current iteration=2450, loss=0.5678606481371739\n",
      "Current iteration=2460, loss=0.48583340434129263\n",
      "Current iteration=2470, loss=0.5978329614669051\n",
      "Current iteration=2480, loss=0.46458474282554824\n",
      "Current iteration=2490, loss=0.45500045780766596\n",
      "Current iteration=2500, loss=0.42769076700981434\n",
      "Current iteration=2510, loss=0.4084916531351337\n",
      "Current iteration=2520, loss=0.43585240593636\n",
      "Current iteration=2530, loss=0.4298997408918695\n",
      "Current iteration=2540, loss=0.4555679455786605\n",
      "Current iteration=2550, loss=0.4578178980326129\n",
      "Current iteration=2560, loss=0.40476729394598526\n",
      "Current iteration=2570, loss=0.5979858836125029\n",
      "Current iteration=2580, loss=0.5008303446220488\n",
      "Current iteration=2590, loss=0.44352727718111895\n",
      "Current iteration=2600, loss=0.48434028192904605\n",
      "Current iteration=2610, loss=0.4747483707778218\n",
      "Current iteration=2620, loss=0.39341755484702423\n",
      "Current iteration=2630, loss=0.4953005862774396\n",
      "Current iteration=2640, loss=0.4066091965217944\n",
      "Current iteration=2650, loss=0.425542894042079\n",
      "Current iteration=2660, loss=0.45433172151352785\n",
      "Current iteration=2670, loss=0.5552788007698062\n",
      "Current iteration=2680, loss=0.5378034875640192\n",
      "Current iteration=2690, loss=0.49262043559872276\n",
      "Current iteration=2700, loss=0.5100665180958777\n",
      "Current iteration=2710, loss=0.5355939975130639\n",
      "Current iteration=2720, loss=0.49704701340973784\n",
      "Current iteration=2730, loss=0.5309490497389511\n",
      "Current iteration=2740, loss=0.45994559703440463\n",
      "Current iteration=2750, loss=0.4907076396389978\n",
      "Current iteration=2760, loss=0.4244115816907308\n",
      "Current iteration=2770, loss=0.44220006177423504\n",
      "Current iteration=2780, loss=0.4206581535042057\n",
      "Current iteration=2790, loss=0.4709983198020907\n",
      "Current iteration=2800, loss=0.44239223928410504\n",
      "Current iteration=2810, loss=0.4416703078589639\n",
      "Current iteration=2820, loss=0.5100771810395893\n",
      "Current iteration=2830, loss=0.4566865678627066\n",
      "Current iteration=2840, loss=0.4760139754709769\n",
      "Current iteration=2850, loss=0.47870932549462014\n",
      "Current iteration=2860, loss=0.5064974177330562\n",
      "Current iteration=2870, loss=0.4880513191174369\n",
      "Current iteration=2880, loss=0.41451961179000585\n",
      "Current iteration=2890, loss=0.4392203019048775\n",
      "Current iteration=2900, loss=0.7634591875175176\n",
      "Current iteration=2910, loss=0.3996115870159735\n",
      "Current iteration=2920, loss=0.4519168445834456\n",
      "Current iteration=2930, loss=0.4646537867083646\n",
      "Current iteration=2940, loss=0.5066631923411323\n",
      "Current iteration=2950, loss=0.5352771109242876\n",
      "Current iteration=2960, loss=0.4856850485364935\n",
      "Current iteration=2970, loss=0.5123030328861152\n",
      "Current iteration=2980, loss=0.4013131093570535\n",
      "Current iteration=2990, loss=0.459872897879523\n",
      "Current iteration=3000, loss=0.4850895472041091\n",
      "Current iteration=3010, loss=0.4226443043285332\n",
      "Current iteration=3020, loss=0.4839779566126589\n",
      "Current iteration=3030, loss=0.5154936887669926\n",
      "Current iteration=3040, loss=0.4209224330623097\n",
      "Current iteration=3050, loss=0.4007597818736568\n",
      "Current iteration=3060, loss=0.41447220000247464\n",
      "Current iteration=3070, loss=0.527243288966653\n",
      "Current iteration=3080, loss=0.5067742923405438\n",
      "Current iteration=3090, loss=0.45645567169651513\n",
      "Current iteration=3100, loss=0.40517124996043613\n",
      "Current iteration=3110, loss=0.4172293266297357\n",
      "Current iteration=3120, loss=0.4816551810060654\n",
      "Current iteration=3130, loss=0.5475397137929563\n",
      "Current iteration=3140, loss=0.3735639139732244\n",
      "Current iteration=3150, loss=0.42209595002393735\n",
      "Current iteration=3160, loss=0.4871376735361696\n",
      "Current iteration=3170, loss=0.4414308543542977\n",
      "Current iteration=3180, loss=0.4847462087281129\n",
      "Current iteration=3190, loss=0.5111485221684571\n",
      "Current iteration=3200, loss=0.5825829652165497\n",
      "Current iteration=3210, loss=0.4543090940553421\n",
      "Current iteration=3220, loss=0.5445822588549332\n",
      "Current iteration=3230, loss=0.45172468423291995\n",
      "Current iteration=3240, loss=0.4612636112785547\n",
      "Current iteration=3250, loss=0.6372533768160007\n",
      "Current iteration=3260, loss=0.40186962150515604\n",
      "Current iteration=3270, loss=0.480815364303032\n",
      "Current iteration=3280, loss=0.469863725154298\n",
      "Current iteration=3290, loss=0.44732566470478846\n",
      "Current iteration=3300, loss=0.4834756933404529\n",
      "Current iteration=3310, loss=0.4479410134483724\n",
      "Current iteration=3320, loss=0.46501553974297805\n",
      "Current iteration=3330, loss=0.4908701103330884\n",
      "Current iteration=3340, loss=0.48445008787541666\n",
      "Current iteration=3350, loss=0.4816285537749967\n",
      "Current iteration=3360, loss=0.4467977089804501\n",
      "Current iteration=3370, loss=0.5373882559365888\n",
      "Current iteration=3380, loss=0.4576410032825548\n",
      "Current iteration=3390, loss=0.4688426425142474\n",
      "Current iteration=3400, loss=0.7800070109073448\n",
      "Current iteration=3410, loss=0.509574179144197\n",
      "Current iteration=3420, loss=0.5152049620235738\n",
      "Current iteration=3430, loss=0.4237460719114628\n",
      "Current iteration=3440, loss=0.45008670464306505\n",
      "Current iteration=3450, loss=0.4647191498770697\n",
      "Current iteration=3460, loss=0.44301962015724095\n",
      "Current iteration=3470, loss=0.48482197972878815\n",
      "Current iteration=3480, loss=0.39039819514890295\n",
      "Current iteration=3490, loss=0.4532375191357039\n",
      "Current iteration=3500, loss=0.4250374489799453\n",
      "Current iteration=3510, loss=0.43129242959665975\n",
      "Current iteration=3520, loss=0.461066230533438\n",
      "Current iteration=3530, loss=0.43365605556130454\n",
      "Current iteration=3540, loss=0.4745179338493565\n",
      "Current iteration=3550, loss=0.3912144201514357\n",
      "Current iteration=3560, loss=0.4968233497256339\n",
      "Current iteration=3570, loss=0.556991245813354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3580, loss=0.3573306100827823\n",
      "Current iteration=3590, loss=0.5042747897044042\n",
      "Current iteration=3600, loss=0.4935947855007581\n",
      "Current iteration=3610, loss=0.4935409684677321\n",
      "Current iteration=3620, loss=0.5299354578738069\n",
      "Current iteration=3630, loss=0.5907523424376869\n",
      "Current iteration=3640, loss=0.5762939437884049\n",
      "Current iteration=3650, loss=0.5090655320781593\n",
      "Current iteration=3660, loss=0.527706124096452\n",
      "Current iteration=3670, loss=0.3982803743471079\n",
      "Current iteration=3680, loss=0.46640264892259536\n",
      "Current iteration=3690, loss=0.4448820697748126\n",
      "Current iteration=3700, loss=0.48783471546115537\n",
      "Current iteration=3710, loss=0.3993163294747096\n",
      "Current iteration=3720, loss=0.4024209057883995\n",
      "Current iteration=3730, loss=0.43640703288874955\n",
      "Current iteration=3740, loss=0.7088161691208454\n",
      "Current iteration=3750, loss=0.4807972620076052\n",
      "Current iteration=3760, loss=0.46519738670497757\n",
      "Current iteration=3770, loss=0.4514854676755771\n",
      "Current iteration=3780, loss=0.5118190597070598\n",
      "Current iteration=3790, loss=0.4369435096466804\n",
      "Current iteration=3800, loss=0.4624612910270656\n",
      "Current iteration=3810, loss=0.5678941693919354\n",
      "Current iteration=3820, loss=0.4257448066675344\n",
      "Current iteration=3830, loss=0.9326222667035382\n",
      "Current iteration=3840, loss=0.42865756778400127\n",
      "Current iteration=3850, loss=0.3395234086222324\n",
      "Current iteration=3860, loss=0.44621408329145035\n",
      "Current iteration=3870, loss=0.4758396343034041\n",
      "Current iteration=3880, loss=0.4014246671411917\n",
      "Current iteration=3890, loss=0.41041158564016716\n",
      "Current iteration=3900, loss=0.44203854036295576\n",
      "Current iteration=3910, loss=0.38936878084667115\n",
      "Current iteration=3920, loss=0.4738386584227586\n",
      "Current iteration=3930, loss=0.45465656917567165\n",
      "Current iteration=3940, loss=0.46514448892955784\n",
      "Current iteration=3950, loss=0.4541599477846544\n",
      "Current iteration=3960, loss=0.3934609867134472\n",
      "Current iteration=3970, loss=0.6035410648592598\n",
      "Current iteration=3980, loss=0.49191204579828934\n",
      "Current iteration=3990, loss=0.5143742889086265\n",
      "Current iteration=4000, loss=0.48778383655881674\n",
      "Current iteration=4010, loss=0.4115367852659635\n",
      "Current iteration=4020, loss=0.39107226423201236\n",
      "Current iteration=4030, loss=0.5088685712431033\n",
      "Current iteration=4040, loss=0.3608273339818252\n",
      "Current iteration=4050, loss=0.4196767375756423\n",
      "Current iteration=4060, loss=0.5208244853954709\n",
      "Current iteration=4070, loss=0.4053135254527009\n",
      "Current iteration=4080, loss=0.49665433002100073\n",
      "Current iteration=4090, loss=0.40638431163712296\n",
      "Current iteration=4100, loss=0.4663105666341933\n",
      "Current iteration=4110, loss=0.42580899755872303\n",
      "Current iteration=4120, loss=0.5065560263091367\n",
      "Current iteration=4130, loss=0.43585342457940995\n",
      "Current iteration=4140, loss=0.4172372168877551\n",
      "Current iteration=4150, loss=0.4654762948549127\n",
      "Current iteration=4160, loss=0.45130268188035866\n",
      "Current iteration=4170, loss=0.48714190298194693\n",
      "Current iteration=4180, loss=0.3811305015372956\n",
      "Current iteration=4190, loss=0.46480155713053306\n",
      "Current iteration=4200, loss=0.4762929073782758\n",
      "Current iteration=4210, loss=0.48050869354596837\n",
      "Current iteration=4220, loss=0.4346369587745014\n",
      "Current iteration=4230, loss=0.4138677508752848\n",
      "Current iteration=4240, loss=0.5049975995668485\n",
      "Current iteration=4250, loss=0.42091379265759654\n",
      "Current iteration=4260, loss=0.3606032975084927\n",
      "Current iteration=4270, loss=0.6722704564656437\n",
      "Current iteration=4280, loss=0.4744785030840184\n",
      "Current iteration=4290, loss=0.45692534679921093\n",
      "Current iteration=4300, loss=0.48654178893455635\n",
      "Current iteration=4310, loss=0.5774019102435418\n",
      "Current iteration=4320, loss=0.5474003086842171\n",
      "Current iteration=4330, loss=0.45173939457360357\n",
      "Current iteration=4340, loss=0.45998057891810973\n",
      "Current iteration=4350, loss=0.5186020726243796\n",
      "Current iteration=4360, loss=0.4864748316350771\n",
      "Current iteration=4370, loss=0.4278854466002266\n",
      "Current iteration=4380, loss=0.43309297663265506\n",
      "Current iteration=4390, loss=0.5346411969080801\n",
      "Current iteration=4400, loss=0.4170065553495644\n",
      "Current iteration=4410, loss=0.4272099602715179\n",
      "Current iteration=4420, loss=0.47517571553917365\n",
      "Current iteration=4430, loss=0.4911736610236016\n",
      "Current iteration=4440, loss=0.51788276829337\n",
      "Current iteration=4450, loss=0.5572177653902907\n",
      "Current iteration=4460, loss=0.3536597424976345\n",
      "Current iteration=4470, loss=0.4893367540262885\n",
      "Current iteration=4480, loss=0.4037847121938053\n",
      "Current iteration=4490, loss=0.44182808159273645\n",
      "Current iteration=4500, loss=0.5719443398641524\n",
      "Current iteration=4510, loss=0.38230068756539287\n",
      "Current iteration=4520, loss=0.39684510018107255\n",
      "Current iteration=4530, loss=0.46391510228996535\n",
      "Current iteration=4540, loss=0.40151114958508743\n",
      "Current iteration=4550, loss=0.5129324483078807\n",
      "Current iteration=4560, loss=0.5431505486554805\n",
      "Current iteration=4570, loss=0.43028592001284766\n",
      "Current iteration=4580, loss=0.4247240002350622\n",
      "Current iteration=4590, loss=0.5518357340802664\n",
      "Current iteration=4600, loss=0.4250427475114699\n",
      "Current iteration=4610, loss=0.4713596446019968\n",
      "Current iteration=4620, loss=0.42889236957529364\n",
      "Current iteration=4630, loss=0.46923399177138114\n",
      "Current iteration=4640, loss=0.5128977984460353\n",
      "Current iteration=4650, loss=0.4152775015127714\n",
      "Current iteration=4660, loss=0.44865842668217826\n",
      "Current iteration=4670, loss=0.4875727593116787\n",
      "Current iteration=4680, loss=0.5073910465438743\n",
      "Current iteration=4690, loss=0.49559458034508547\n",
      "Current iteration=4700, loss=0.5238819658295719\n",
      "Current iteration=4710, loss=0.476984302132483\n",
      "Current iteration=4720, loss=0.4575079902818111\n",
      "Current iteration=4730, loss=0.41547027834885575\n",
      "Current iteration=4740, loss=0.49398940519238715\n",
      "Current iteration=4750, loss=0.5040706758635594\n",
      "Current iteration=4760, loss=0.5413185649046808\n",
      "Current iteration=4770, loss=0.4085269877916759\n",
      "Current iteration=4780, loss=0.4674287224534319\n",
      "Current iteration=4790, loss=0.5143263566769085\n",
      "Current iteration=4800, loss=0.4675218695982768\n",
      "Current iteration=4810, loss=0.4287489626877965\n",
      "Current iteration=4820, loss=0.4823683742560552\n",
      "Current iteration=4830, loss=0.4157985501994312\n",
      "Current iteration=4840, loss=0.48413001355463464\n",
      "Current iteration=4850, loss=0.5071770021198423\n",
      "Current iteration=4860, loss=0.47923241619043105\n",
      "Current iteration=4870, loss=0.43643147213178834\n",
      "Current iteration=4880, loss=0.581616757486658\n",
      "Current iteration=4890, loss=0.5730281826488475\n",
      "Current iteration=4900, loss=0.49934561526926474\n",
      "Current iteration=4910, loss=0.5053292330067013\n",
      "Current iteration=4920, loss=0.369483891557932\n",
      "Current iteration=4930, loss=0.40598461660508717\n",
      "Current iteration=4940, loss=0.42509407045793546\n",
      "Current iteration=4950, loss=0.4479858815216516\n",
      "Current iteration=4960, loss=0.4815563140940733\n",
      "Current iteration=4970, loss=0.4561631186941131\n",
      "Current iteration=4980, loss=0.5031557210317879\n",
      "Current iteration=4990, loss=0.490200633776098\n",
      "Current iteration=5000, loss=0.4689925068568615\n",
      "Current iteration=5010, loss=0.4601445615330489\n",
      "Current iteration=5020, loss=0.512121701374378\n",
      "Current iteration=5030, loss=0.5241280341689896\n",
      "Current iteration=5040, loss=0.3976567154599468\n",
      "Current iteration=5050, loss=0.480707855824252\n",
      "Current iteration=5060, loss=0.39085182831795473\n",
      "Current iteration=5070, loss=0.40869914961980797\n",
      "Current iteration=5080, loss=0.5516177398774471\n",
      "Current iteration=5090, loss=0.38315334892360314\n",
      "Current iteration=5100, loss=0.443507340292207\n",
      "Current iteration=5110, loss=0.4606617091764894\n",
      "Current iteration=5120, loss=0.4531777862709496\n",
      "Current iteration=5130, loss=0.41723859714666206\n",
      "Current iteration=5140, loss=0.5282975548709175\n",
      "Current iteration=5150, loss=0.4119053871707283\n",
      "Current iteration=5160, loss=0.43366648132017543\n",
      "Current iteration=5170, loss=0.4909645701476344\n",
      "Current iteration=5180, loss=0.48681313848486646\n",
      "Current iteration=5190, loss=0.4221332208224206\n",
      "Current iteration=5200, loss=0.4305189226388945\n",
      "Current iteration=5210, loss=0.39630967126863753\n",
      "Current iteration=5220, loss=0.47144852527960096\n",
      "Current iteration=5230, loss=0.3757343530537248\n",
      "Current iteration=5240, loss=0.4017610508324092\n",
      "Current iteration=5250, loss=0.48242969813079095\n",
      "Current iteration=5260, loss=0.38426720087315225\n",
      "Current iteration=5270, loss=0.3888562540680364\n",
      "Current iteration=5280, loss=0.4367648052169446\n",
      "Current iteration=5290, loss=0.4847150365511569\n",
      "Current iteration=5300, loss=0.5213372438441517\n",
      "Current iteration=5310, loss=0.4037753161824858\n",
      "Current iteration=5320, loss=0.4558499745034528\n",
      "Current iteration=5330, loss=0.41899095581425366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=5340, loss=0.38410809120204453\n",
      "Current iteration=5350, loss=0.4844795669476185\n",
      "Current iteration=5360, loss=0.429409794023379\n",
      "Current iteration=5370, loss=0.4222327130108053\n",
      "Current iteration=5380, loss=0.4177333290784462\n",
      "Current iteration=5390, loss=0.4206226307856182\n",
      "Current iteration=5400, loss=0.46788676107077903\n",
      "Current iteration=5410, loss=0.5018722714377205\n",
      "Current iteration=5420, loss=0.5086667110423053\n",
      "Current iteration=5430, loss=0.48390170551488254\n",
      "Current iteration=5440, loss=0.4995459259294463\n",
      "Current iteration=5450, loss=0.5108377628033135\n",
      "Current iteration=5460, loss=0.48704702526167004\n",
      "Current iteration=5470, loss=0.4584085841968731\n",
      "Current iteration=5480, loss=0.4742776536162466\n",
      "Current iteration=5490, loss=0.529201775324225\n",
      "Current iteration=5500, loss=0.47145815178828776\n",
      "Current iteration=5510, loss=0.39397553800953294\n",
      "Current iteration=5520, loss=0.4985304039743285\n",
      "Current iteration=5530, loss=0.4299031805339832\n",
      "Current iteration=5540, loss=0.4972515905191145\n",
      "Current iteration=5550, loss=0.4745033380513196\n",
      "Current iteration=5560, loss=0.37580007707959845\n",
      "Current iteration=5570, loss=0.45067028624458394\n",
      "Current iteration=5580, loss=0.39796831174900205\n",
      "Current iteration=5590, loss=0.40622141341227747\n",
      "Current iteration=5600, loss=0.4836441856793695\n",
      "Current iteration=5610, loss=0.5030222724331564\n",
      "Current iteration=5620, loss=0.4470718795363181\n",
      "Current iteration=5630, loss=0.5249246152067724\n",
      "Current iteration=5640, loss=0.5689933871484222\n",
      "Current iteration=5650, loss=0.5785445796142699\n",
      "Current iteration=5660, loss=0.40496405004908903\n",
      "Current iteration=5670, loss=0.7569838561472872\n",
      "Current iteration=5680, loss=0.44063445874815055\n",
      "Current iteration=5690, loss=0.507484724389387\n",
      "Current iteration=5700, loss=0.435282856396266\n",
      "Current iteration=5710, loss=0.5704567601369326\n",
      "Current iteration=5720, loss=0.4984426692083699\n",
      "Current iteration=5730, loss=0.4391275660433716\n",
      "Current iteration=5740, loss=0.5161280227127968\n",
      "Current iteration=5750, loss=0.4364425117248537\n",
      "Current iteration=5760, loss=0.3707282415067789\n",
      "Current iteration=5770, loss=0.4491004589612176\n",
      "Current iteration=5780, loss=0.44906028596497505\n",
      "Current iteration=5790, loss=0.48125316365260923\n",
      "Current iteration=5800, loss=0.4522513457993436\n",
      "Current iteration=5810, loss=0.4221873670964331\n",
      "Current iteration=5820, loss=0.5143725438140478\n",
      "Current iteration=5830, loss=0.40126905112906625\n",
      "Current iteration=5840, loss=0.4628425245542005\n",
      "Current iteration=5850, loss=0.38420642004679734\n",
      "Current iteration=5860, loss=0.40762622617875294\n",
      "Current iteration=5870, loss=0.3977745747503933\n",
      "Current iteration=5880, loss=0.38417937155167914\n",
      "Current iteration=5890, loss=0.44269692695128215\n",
      "Current iteration=5900, loss=0.5447982467592432\n",
      "Current iteration=5910, loss=0.4079201491347699\n",
      "Current iteration=5920, loss=0.4708342678194704\n",
      "Current iteration=5930, loss=0.3855197897214387\n",
      "Current iteration=5940, loss=0.5346277211636683\n",
      "Current iteration=5950, loss=0.5147119826815733\n",
      "Current iteration=5960, loss=0.45694039851454\n",
      "Current iteration=5970, loss=0.4762758781853105\n",
      "Current iteration=5980, loss=0.4403679860973105\n",
      "Current iteration=5990, loss=0.3985090289516969\n",
      "Current iteration=6000, loss=0.47372318353161785\n",
      "Current iteration=6010, loss=0.49630835038471716\n",
      "Current iteration=6020, loss=0.4699736639045776\n",
      "Current iteration=6030, loss=0.36994774702148836\n",
      "Current iteration=6040, loss=0.43405111685164693\n",
      "Current iteration=6050, loss=0.5474962019926835\n",
      "Current iteration=6060, loss=0.49100233837077545\n",
      "Current iteration=6070, loss=0.4376760973105943\n",
      "Current iteration=6080, loss=0.4260203146524315\n",
      "Current iteration=6090, loss=0.47228280839997067\n",
      "Current iteration=6100, loss=0.5529538628877894\n",
      "Current iteration=6110, loss=0.46920629031591077\n",
      "Current iteration=6120, loss=0.40439209367590634\n",
      "Current iteration=6130, loss=0.501884436382991\n",
      "Current iteration=6140, loss=0.36623985452613217\n",
      "Current iteration=6150, loss=0.46707889580024076\n",
      "Current iteration=6160, loss=0.5487484276883303\n",
      "Current iteration=6170, loss=0.4298049212975466\n",
      "Current iteration=6180, loss=0.42944946082053337\n",
      "Current iteration=6190, loss=0.4735582755684596\n",
      "Current iteration=6200, loss=0.5700404641040402\n",
      "Current iteration=6210, loss=0.47991637612959054\n",
      "Current iteration=6220, loss=0.6271134587569752\n",
      "Current iteration=6230, loss=0.45113255596753205\n",
      "Current iteration=6240, loss=0.4481972809569438\n",
      "Current iteration=6250, loss=0.4217700188994572\n",
      "Current iteration=6260, loss=0.3966472480461649\n",
      "Current iteration=6270, loss=0.421248064600368\n",
      "Current iteration=6280, loss=0.4182604108761552\n",
      "Current iteration=6290, loss=0.45309865409754435\n",
      "Current iteration=6300, loss=0.4505599364600266\n",
      "Current iteration=6310, loss=0.3993286140093663\n",
      "Current iteration=6320, loss=0.6190323933331748\n",
      "Current iteration=6330, loss=0.5058619568106607\n",
      "Current iteration=6340, loss=0.4354381384077247\n",
      "Current iteration=6350, loss=0.47954477901642045\n",
      "Current iteration=6360, loss=0.4671794268672695\n",
      "Current iteration=6370, loss=0.38171978208445223\n",
      "Current iteration=6380, loss=0.5047707279826248\n",
      "Current iteration=6390, loss=0.39270413896721534\n",
      "Current iteration=6400, loss=0.4212619938993868\n",
      "Current iteration=6410, loss=0.4628725764840231\n",
      "Current iteration=6420, loss=0.5639675112793078\n",
      "Current iteration=6430, loss=0.5302200789873359\n",
      "Current iteration=6440, loss=0.4888886551664168\n",
      "Current iteration=6450, loss=0.51276083052031\n",
      "Current iteration=6460, loss=0.5538484036584632\n",
      "Current iteration=6470, loss=0.5019373893454905\n",
      "Current iteration=6480, loss=0.5394170199001338\n",
      "Current iteration=6490, loss=0.45658287539049197\n",
      "Current iteration=6500, loss=0.48899758184965697\n",
      "Current iteration=6510, loss=0.41514082871815766\n",
      "Current iteration=6520, loss=0.4390823241440173\n",
      "Current iteration=6530, loss=0.41378744104005494\n",
      "Current iteration=6540, loss=0.48925800206889924\n",
      "Current iteration=6550, loss=0.43737485432970763\n",
      "Current iteration=6560, loss=0.43978863964413994\n",
      "Current iteration=6570, loss=0.5095017491518499\n",
      "Current iteration=6580, loss=0.45183657735461624\n",
      "Current iteration=6590, loss=0.47592128989589955\n",
      "Current iteration=6600, loss=0.48401405533539\n",
      "Current iteration=6610, loss=0.5083234560498416\n",
      "Current iteration=6620, loss=0.48548595438046704\n",
      "Current iteration=6630, loss=0.4063301473207751\n",
      "Current iteration=6640, loss=0.42966013137017944\n",
      "Current iteration=6650, loss=0.7838226508664783\n",
      "Current iteration=6660, loss=0.39189625947177814\n",
      "Current iteration=6670, loss=0.44135225256595506\n",
      "Current iteration=6680, loss=0.46958068145855303\n",
      "Current iteration=6690, loss=0.5006442507867422\n",
      "Current iteration=6700, loss=0.543675636207817\n",
      "Current iteration=6710, loss=0.48566974211038255\n",
      "Current iteration=6720, loss=0.5109361587208269\n",
      "Current iteration=6730, loss=0.39047825219658017\n",
      "Current iteration=6740, loss=0.45348378270512085\n",
      "Current iteration=6750, loss=0.47999238313093706\n",
      "Current iteration=6760, loss=0.4118283081703266\n",
      "Current iteration=6770, loss=0.4741419042240426\n",
      "Current iteration=6780, loss=0.5275418842162485\n",
      "Current iteration=6790, loss=0.4187082532648816\n",
      "Current iteration=6800, loss=0.39215611307647125\n",
      "Current iteration=6810, loss=0.4066602321053633\n",
      "Current iteration=6820, loss=0.5261379701030997\n",
      "Current iteration=6830, loss=0.5036516782196173\n",
      "Current iteration=6840, loss=0.45434241780237533\n",
      "Current iteration=6850, loss=0.40387967391969226\n",
      "Current iteration=6860, loss=0.40924097255013436\n",
      "Current iteration=6870, loss=0.48289754019851877\n",
      "Current iteration=6880, loss=0.5570636790083119\n",
      "Current iteration=6890, loss=0.3625357788067016\n",
      "Current iteration=6900, loss=0.4213120974602704\n",
      "Current iteration=6910, loss=0.48982091542287187\n",
      "Current iteration=6920, loss=0.44181933010471264\n",
      "Current iteration=6930, loss=0.49158697434556814\n",
      "Current iteration=6940, loss=0.5189472716195105\n",
      "Current iteration=6950, loss=0.5900104171566937\n",
      "Current iteration=6960, loss=0.45440406573702996\n",
      "Current iteration=6970, loss=0.5495027831629491\n",
      "Current iteration=6980, loss=0.44467611049234357\n",
      "Current iteration=6990, loss=0.4576324594019778\n",
      "Current iteration=7000, loss=0.6383957266534517\n",
      "Current iteration=7010, loss=0.3989460921708546\n",
      "Current iteration=7020, loss=0.4859492226249311\n",
      "Current iteration=7030, loss=0.4620939413485307\n",
      "Current iteration=7040, loss=0.4456680580241422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=7050, loss=0.4862304992638206\n",
      "Current iteration=7060, loss=0.44586872785656895\n",
      "Current iteration=7070, loss=0.4603161335862172\n",
      "Current iteration=7080, loss=0.4962424172045404\n",
      "Current iteration=7090, loss=0.4860557200375573\n",
      "Current iteration=7100, loss=0.48151064357592027\n",
      "Current iteration=7110, loss=0.45693415591205605\n",
      "Current iteration=7120, loss=0.5454441427942668\n",
      "Current iteration=7130, loss=0.4616399090125372\n",
      "Current iteration=7140, loss=0.4633243350243408\n",
      "Current iteration=7150, loss=0.7973771913754015\n",
      "Current iteration=7160, loss=0.5185463605499349\n",
      "Current iteration=7170, loss=0.5100345974444106\n",
      "Current iteration=7180, loss=0.4201659875896503\n",
      "Current iteration=7190, loss=0.4454816248835376\n",
      "Current iteration=7200, loss=0.46460999329291125\n",
      "Current iteration=7210, loss=0.4371354725209297\n",
      "Current iteration=7220, loss=0.48544625722714735\n",
      "Current iteration=7230, loss=0.38566160187351756\n",
      "Current iteration=7240, loss=0.44971101910976075\n",
      "Current iteration=7250, loss=0.4261187724616742\n",
      "Current iteration=7260, loss=0.4259087846072566\n",
      "Current iteration=7270, loss=0.45907586332718514\n",
      "Current iteration=7280, loss=0.4315729464902916\n",
      "Current iteration=7290, loss=0.47644489263798634\n",
      "Current iteration=7300, loss=0.3812475566540066\n",
      "Current iteration=7310, loss=0.5067623649411417\n",
      "Current iteration=7320, loss=0.5620752672381384\n",
      "Current iteration=7330, loss=0.3511214462891483\n",
      "Current iteration=7340, loss=0.49743553871519613\n",
      "Current iteration=7350, loss=0.48767094818252443\n",
      "Current iteration=7360, loss=0.4939806480903824\n",
      "Current iteration=7370, loss=0.5337426392345019\n",
      "Current iteration=7380, loss=0.6019904325957407\n",
      "Current iteration=7390, loss=0.5815576783751929\n",
      "Current iteration=7400, loss=0.5144248442956739\n",
      "Current iteration=7410, loss=0.5301113772455166\n",
      "Current iteration=7420, loss=0.39410916976925564\n",
      "Current iteration=7430, loss=0.45710624766021274\n",
      "Current iteration=7440, loss=0.44264628616038826\n",
      "Current iteration=7450, loss=0.49401345883149117\n",
      "Current iteration=7460, loss=0.3934026131850628\n",
      "Current iteration=7470, loss=0.39670963553266775\n",
      "Current iteration=7480, loss=0.43361223425071943\n",
      "Current iteration=7490, loss=0.7266822633911977\n",
      "Current iteration=7500, loss=0.48046594453013375\n",
      "Current iteration=7510, loss=0.46845932000596185\n",
      "Current iteration=7520, loss=0.4526380217698487\n",
      "Current iteration=7530, loss=0.5128798163526203\n",
      "Current iteration=7540, loss=0.4358096567429648\n",
      "Current iteration=7550, loss=0.46010254169838705\n",
      "Current iteration=7560, loss=0.572799015672046\n",
      "Current iteration=7570, loss=0.4178038003383392\n",
      "Current iteration=7580, loss=0.9535968397435556\n",
      "Current iteration=7590, loss=0.4279868224957869\n",
      "Current iteration=7600, loss=0.3333502851913442\n",
      "Current iteration=7610, loss=0.4473910566524013\n",
      "Current iteration=7620, loss=0.47691491139395503\n",
      "Current iteration=7630, loss=0.39926398684484615\n",
      "Current iteration=7640, loss=0.40726056734797544\n",
      "Current iteration=7650, loss=0.44199137450504905\n",
      "Current iteration=7660, loss=0.3884933417953196\n",
      "Current iteration=7670, loss=0.47428066770186605\n",
      "Current iteration=7680, loss=0.4528337908357997\n",
      "Current iteration=7690, loss=0.4656462669800164\n",
      "Current iteration=7700, loss=0.45352879281391095\n",
      "Current iteration=7710, loss=0.39025555041421256\n",
      "Current iteration=7720, loss=0.6023977076355309\n",
      "Current iteration=7730, loss=0.49343525605612015\n",
      "Current iteration=7740, loss=0.5164872675506086\n",
      "Current iteration=7750, loss=0.4866837317659913\n",
      "Current iteration=7760, loss=0.40866850502371177\n",
      "Current iteration=7770, loss=0.3884797049357034\n",
      "Current iteration=7780, loss=0.5136511238442949\n",
      "Current iteration=7790, loss=0.35699584048724026\n",
      "Current iteration=7800, loss=0.4207171107917759\n",
      "Current iteration=7810, loss=0.5243711338763838\n",
      "Current iteration=7820, loss=0.40274389356198653\n",
      "Current iteration=7830, loss=0.4976008597678447\n",
      "Current iteration=7840, loss=0.4033259176394035\n",
      "Current iteration=7850, loss=0.466852851864623\n",
      "Current iteration=7860, loss=0.4304997387749592\n",
      "Current iteration=7870, loss=0.5067575295010593\n",
      "Current iteration=7880, loss=0.4335104122522466\n",
      "Current iteration=7890, loss=0.4159213938877876\n",
      "Current iteration=7900, loss=0.46501890183186434\n",
      "Current iteration=7910, loss=0.4525173329638751\n",
      "Current iteration=7920, loss=0.4892044121564218\n",
      "Current iteration=7930, loss=0.3780987566554866\n",
      "Current iteration=7940, loss=0.467509786785298\n",
      "Current iteration=7950, loss=0.47789155789837884\n",
      "Current iteration=7960, loss=0.47859200409990516\n",
      "Current iteration=7970, loss=0.43256525472985563\n",
      "Current iteration=7980, loss=0.4112140188948096\n",
      "Current iteration=7990, loss=0.5064501447959951\n",
      "Current iteration=8000, loss=0.4214268683196727\n",
      "Current iteration=8010, loss=0.3588525758363302\n",
      "Current iteration=8020, loss=0.6812420116737963\n",
      "Current iteration=8030, loss=0.47571374057519766\n",
      "Current iteration=8040, loss=0.455855950715319\n",
      "Current iteration=8050, loss=0.4865899845583334\n",
      "Current iteration=8060, loss=0.5814574984059135\n",
      "Current iteration=8070, loss=0.5527285932744687\n",
      "Current iteration=8080, loss=0.4503759907357484\n",
      "Current iteration=8090, loss=0.458317119542229\n",
      "Current iteration=8100, loss=0.520522990858275\n",
      "Current iteration=8110, loss=0.48442727254945306\n",
      "Current iteration=8120, loss=0.4266999470939873\n",
      "Current iteration=8130, loss=0.4321949957197724\n",
      "Current iteration=8140, loss=0.537025868268799\n",
      "Current iteration=8150, loss=0.413303793957482\n",
      "Current iteration=8160, loss=0.42656260168262317\n",
      "Current iteration=8170, loss=0.4748328477246417\n",
      "Current iteration=8180, loss=0.49214563747316503\n",
      "Current iteration=8190, loss=0.5215615176780071\n",
      "Current iteration=8200, loss=0.5598731532849421\n",
      "Current iteration=8210, loss=0.3505522557192265\n",
      "Current iteration=8220, loss=0.4886487247360588\n",
      "Current iteration=8230, loss=0.40291792495418455\n",
      "Current iteration=8240, loss=0.44274260978955304\n",
      "Current iteration=8250, loss=0.5766596269952017\n",
      "Current iteration=8260, loss=0.3816921876443775\n",
      "Current iteration=8270, loss=0.3947217575768072\n",
      "Current iteration=8280, loss=0.4631048686333545\n",
      "Current iteration=8290, loss=0.40082009331696816\n",
      "Current iteration=8300, loss=0.5143569193411047\n",
      "Current iteration=8310, loss=0.5463042997411335\n",
      "Current iteration=8320, loss=0.4303091057058919\n",
      "Current iteration=8330, loss=0.42472466697061534\n",
      "Current iteration=8340, loss=0.5553992936609744\n",
      "Current iteration=8350, loss=0.42464993488098635\n",
      "Current iteration=8360, loss=0.4706814652823006\n",
      "Current iteration=8370, loss=0.428120764262295\n",
      "Current iteration=8380, loss=0.4701960214909125\n",
      "Current iteration=8390, loss=0.5158232686080904\n",
      "Current iteration=8400, loss=0.4137842284869306\n",
      "Current iteration=8410, loss=0.44932413059870996\n",
      "Current iteration=8420, loss=0.4873453719094909\n",
      "Current iteration=8430, loss=0.5083736432949253\n",
      "Current iteration=8440, loss=0.49616240855348925\n",
      "Current iteration=8450, loss=0.523829713339677\n",
      "Current iteration=8460, loss=0.47627566047666486\n",
      "Current iteration=8470, loss=0.4571793190877726\n",
      "Current iteration=8480, loss=0.41506645893642036\n",
      "Current iteration=8490, loss=0.49593987734262585\n",
      "Current iteration=8500, loss=0.5038637872058583\n",
      "Current iteration=8510, loss=0.5435340684707554\n",
      "Current iteration=8520, loss=0.4079544229495468\n",
      "Current iteration=8530, loss=0.4675487665161115\n",
      "Current iteration=8540, loss=0.5157946025476196\n",
      "Current iteration=8550, loss=0.46731429938352276\n",
      "Current iteration=8560, loss=0.42840743574390194\n",
      "Current iteration=8570, loss=0.48386142935226606\n",
      "Current iteration=8580, loss=0.4146231242539881\n",
      "Current iteration=8590, loss=0.4845606791214073\n",
      "Current iteration=8600, loss=0.5074566558333391\n",
      "Current iteration=8610, loss=0.4803119309732463\n",
      "Current iteration=8620, loss=0.43479996669745347\n",
      "Current iteration=8630, loss=0.5841198874285775\n",
      "Current iteration=8640, loss=0.5745283104385284\n",
      "Current iteration=8650, loss=0.4995993905327322\n",
      "Current iteration=8660, loss=0.5061509370302578\n",
      "Current iteration=8670, loss=0.36867895811131296\n",
      "Current iteration=8680, loss=0.4059612244130185\n",
      "Current iteration=8690, loss=0.42502392254001947\n",
      "Current iteration=8700, loss=0.44680962503612937\n",
      "Current iteration=8710, loss=0.48097831498780863\n",
      "Current iteration=8720, loss=0.45527880080525196\n",
      "Current iteration=8730, loss=0.5043449187500442\n",
      "Current iteration=8740, loss=0.4907790408394311\n",
      "Current iteration=8750, loss=0.4689474159595857\n",
      "Current iteration=8760, loss=0.459703130926185\n",
      "Current iteration=8770, loss=0.5127710339997038\n",
      "Current iteration=8780, loss=0.5252831209813339\n",
      "Current iteration=8790, loss=0.39675507556721534\n",
      "Current iteration=8800, loss=0.48047522693440115\n",
      "Current iteration=8810, loss=0.3898329099555264\n",
      "Current iteration=8820, loss=0.4071704406353124\n",
      "Current iteration=8830, loss=0.5536858994199394\n",
      "Current iteration=8840, loss=0.3828700389475837\n",
      "Current iteration=8850, loss=0.44403238515674465\n",
      "Current iteration=8860, loss=0.4598482612980076\n",
      "Current iteration=8870, loss=0.45402177039583014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=8880, loss=0.41716912325014277\n",
      "Current iteration=8890, loss=0.5295015686533016\n",
      "Current iteration=8900, loss=0.4122578362435113\n",
      "Current iteration=8910, loss=0.4334191609794184\n",
      "Current iteration=8920, loss=0.49066128582153135\n",
      "Current iteration=8930, loss=0.4879603138969147\n",
      "Current iteration=8940, loss=0.4221975036941336\n",
      "Current iteration=8950, loss=0.4309698371225709\n",
      "Current iteration=8960, loss=0.3959854584427893\n",
      "Current iteration=8970, loss=0.4724902035854164\n",
      "Current iteration=8980, loss=0.37499940952208327\n",
      "Current iteration=8990, loss=0.4016294053452403\n",
      "Current iteration=9000, loss=0.48178934877915125\n",
      "Current iteration=9010, loss=0.383190376958959\n",
      "Current iteration=9020, loss=0.3881657694030306\n",
      "Current iteration=9030, loss=0.4372567321950032\n",
      "Current iteration=9040, loss=0.48532119290846754\n",
      "Current iteration=9050, loss=0.5220829597815617\n",
      "Current iteration=9060, loss=0.4034659344151988\n",
      "Current iteration=9070, loss=0.4561296031645976\n",
      "Current iteration=9080, loss=0.41826738872316704\n",
      "Current iteration=9090, loss=0.38334251060045504\n",
      "Current iteration=9100, loss=0.4850829919025453\n",
      "Current iteration=9110, loss=0.42928807799627544\n",
      "Current iteration=9120, loss=0.42131144147717925\n",
      "Current iteration=9130, loss=0.4164176623672347\n",
      "Current iteration=9140, loss=0.4201727965594227\n",
      "Current iteration=9150, loss=0.46804683050752677\n",
      "Current iteration=9160, loss=0.502462027837391\n",
      "Current iteration=9170, loss=0.5083551246686637\n",
      "Current iteration=9180, loss=0.48332220893576777\n",
      "Current iteration=9190, loss=0.5019582237666144\n",
      "Current iteration=9200, loss=0.5110218442801201\n",
      "Current iteration=9210, loss=0.4871384526592524\n",
      "Current iteration=9220, loss=0.458343450780481\n",
      "Current iteration=9230, loss=0.4738870756587927\n",
      "Current iteration=9240, loss=0.5304084240668075\n",
      "Current iteration=9250, loss=0.4712728864497981\n",
      "Current iteration=9260, loss=0.39376313346962893\n",
      "Current iteration=9270, loss=0.4987740382578378\n",
      "Current iteration=9280, loss=0.4292979052324711\n",
      "Current iteration=9290, loss=0.49798761549896803\n",
      "Current iteration=9300, loss=0.47480325297737136\n",
      "Current iteration=9310, loss=0.3747122555730028\n",
      "Current iteration=9320, loss=0.4503335012770935\n",
      "Current iteration=9330, loss=0.3970243579934303\n",
      "Current iteration=9340, loss=0.40510575040674335\n",
      "Current iteration=9350, loss=0.48449735114032\n",
      "Current iteration=9360, loss=0.5035421128674897\n",
      "Current iteration=9370, loss=0.44760854520322035\n",
      "Current iteration=9380, loss=0.5251870598985849\n",
      "Current iteration=9390, loss=0.5694942111297374\n",
      "Current iteration=9400, loss=0.5793510936062147\n",
      "Current iteration=9410, loss=0.40579156536910316\n",
      "Current iteration=9420, loss=0.7600228382196004\n",
      "Current iteration=9430, loss=0.44030096957220805\n",
      "Current iteration=9440, loss=0.507003216585746\n",
      "Current iteration=9450, loss=0.43466703207028046\n",
      "Current iteration=9460, loss=0.5711471639159601\n",
      "Current iteration=9470, loss=0.49930035954211877\n",
      "Current iteration=9480, loss=0.4392548045421554\n",
      "Current iteration=9490, loss=0.5169450772632325\n",
      "Current iteration=9500, loss=0.4365333185124355\n",
      "Current iteration=9510, loss=0.37014187523288833\n",
      "Current iteration=9520, loss=0.44863135759047945\n",
      "Current iteration=9530, loss=0.44922398029378896\n",
      "Current iteration=9540, loss=0.48167655901588885\n",
      "Current iteration=9550, loss=0.45249863983293426\n",
      "Current iteration=9560, loss=0.4223930232508106\n",
      "Current iteration=9570, loss=0.514761492629321\n",
      "Current iteration=9580, loss=0.40102813053316844\n",
      "Current iteration=9590, loss=0.46211629108747593\n",
      "Current iteration=9600, loss=0.38437046852276535\n",
      "Current iteration=9610, loss=0.4071395297268097\n",
      "Current iteration=9620, loss=0.39779614032131283\n",
      "Current iteration=9630, loss=0.3840099391714121\n",
      "Current iteration=9640, loss=0.4424859684920473\n",
      "Current iteration=9650, loss=0.545678484981501\n",
      "Current iteration=9660, loss=0.40755093351483274\n",
      "Current iteration=9670, loss=0.47135477457669744\n",
      "Current iteration=9680, loss=0.38524491896399193\n",
      "Current iteration=9690, loss=0.5346456227901122\n",
      "Current iteration=9700, loss=0.5145156221989864\n",
      "Current iteration=9710, loss=0.4568378399503026\n",
      "Current iteration=9720, loss=0.47647749382477983\n",
      "Current iteration=9730, loss=0.4402796553374315\n",
      "Current iteration=9740, loss=0.398538270492925\n",
      "Current iteration=9750, loss=0.47362467427923177\n",
      "Current iteration=9760, loss=0.496759744136163\n",
      "Current iteration=9770, loss=0.46980568452229243\n",
      "Current iteration=9780, loss=0.3696302225983425\n",
      "Current iteration=9790, loss=0.4335572798056716\n",
      "Current iteration=9800, loss=0.5482214559946809\n",
      "Current iteration=9810, loss=0.4913125425786425\n",
      "Current iteration=9820, loss=0.43784825870265764\n",
      "Current iteration=9830, loss=0.42613909119453836\n",
      "Current iteration=9840, loss=0.4722609072469803\n",
      "Current iteration=9850, loss=0.5537746580233488\n",
      "Current iteration=9860, loss=0.4691579481021513\n",
      "Current iteration=9870, loss=0.4041434434457068\n",
      "Current iteration=9880, loss=0.5020172655585279\n",
      "Current iteration=9890, loss=0.36596752722742615\n",
      "Current iteration=9900, loss=0.4672994726994241\n",
      "Current iteration=9910, loss=0.5489068687722902\n",
      "Current iteration=9920, loss=0.4296385567203567\n",
      "Current iteration=9930, loss=0.42937289291207587\n",
      "Current iteration=9940, loss=0.4738579429323685\n",
      "Current iteration=9950, loss=0.5697462766395001\n",
      "Current iteration=9960, loss=0.48013394004540255\n",
      "Current iteration=9970, loss=0.6282277307426913\n",
      "Current iteration=9980, loss=0.4510063567480605\n",
      "Current iteration=9990, loss=0.4481785892529678\n",
      "[0.778272]\n"
     ]
    }
   ],
   "source": [
    "degree = 2\n",
    "lambdas = [0.01]\n",
    "method = 'newton'\n",
    "validationy[np.nonzero(validationy == 0)] = -1\n",
    "\n",
    "tx = build_poly(trainx, degree)\n",
    "validationtx = build_poly(validationx, degree)\n",
    "\n",
    "weight = np.zeros(tx.shape[1])\n",
    "accuracy = np.zeros(len(lambdas))\n",
    "w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    w = running_gradient(trainyas, tx, w, lambda_, method)\n",
    "    np.append(weight,w)\n",
    "    predictions = predict_labels(w, validationtx)\n",
    "    accuracy[ind] = calculate_classification_accuracy(validationy,predictions)\n",
    "    \n",
    "weights = w\n",
    "\n",
    "#plt.plot(lambdas,accuracy)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782224\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_labels(w, validationtx)\n",
    "\n",
    "accuracy = calculate_classification_accuracy(validationy,predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'C:/Users/joeld/Desktop/EPFL/machine learning/AIAIaie/data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_test = remove_features_with_too_many_missing_values(tX_test,0.66)\n",
    "Data_test = replace_missing_values_with_global_mean(Data_test)\n",
    "ZData_test = Z_score_of_each_feature(Data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'C:/Users/joeld/Desktop/EPFL/machine learning/AIAIaie/data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, ZData_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test implementations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-509302a33af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainyas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\EPFL\\machine learning\\AIAIaie\\scripts\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;31m# get loss and update w using regularized logistic regression.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\EPFL\\machine learning\\AIAIaie\\scripts\\functions_for_implementations.py\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[1;34m(y, tx, w, gamma, lambda_, batch_size)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;31m# ***************************************************\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m#calculate loss and gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m#update the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\EPFL\\machine learning\\AIAIaie\\scripts\\functions_for_implementations.py\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[1;34m(y, tx, w, lambda_, batch_size)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;31m# ***************************************************\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;31m#calculate gradient using minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mminibatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_tx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_tx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_tx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\EPFL\\machine learning\\AIAIaie\\scripts\\functions_for_implementations.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degree = 1\n",
    "lambda_ = 0.01\n",
    "\n",
    "validationy[np.nonzero(validationy == 0)] = -1\n",
    "max_iters = 1000\n",
    "gamma = 1e-4\n",
    "\n",
    "tx = build_poly(trainx, degree)\n",
    "validationtx = build_poly(validationx, degree)\n",
    "\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "w, loss = reg_logistic_regression(trainyas, tx, lambda_ , initial_w, max_iters, gamma)\n",
    "\n",
    "print(w)\n",
    "predictions = predict_labels(w, validationtx)\n",
    "accuracy = calculate_classification_accuracy(validationy,predictions)\n",
    "    \n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "ind = np.linspace(0,4-1,4, dtype = np.int64)\n",
    "a = np.delete(ind,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
